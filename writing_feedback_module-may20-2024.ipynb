{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80199e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take text as input\n",
    "# tokenize into sentences\n",
    "# find errors in each sentence\n",
    "# show with replaced text\n",
    "# show info about types of mistakes, vocabulary level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28f1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7826a3eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897971b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efabc858",
   "metadata": {},
   "outputs": [],
   "source": [
    "cefr_vocab = pd.read_csv('./cefr-vocab-cefrj-octanove.csv')\n",
    "cefr_dict = {k : v for k,v in cefr_vocab[['headword', 'CEFR']].values}\n",
    "word_set = set(cefr_vocab.headword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f19df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to c:\\users\\vaibhav\\appdata\\local\\temp\\pip-req-build-_ywjkkt9\n",
      "  Resolved https://github.com/PrithivirajDamodaran/Gramformer.git to commit 23425cd2e98a919384cab6156af8adf1c9d0639a\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (4.24.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.2.0)\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.25.1)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.18.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.11.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (2022.11.0)\n",
      "Requirement already satisfied: errant in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from gramformer==1.0) (3.0.0)\n",
      "Requirement already satisfied: spacy<4,>=3.2.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from errant->gramformer==1.0) (3.7.4)\n",
      "Requirement already satisfied: rapidfuzz>=3.4.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from errant->gramformer==1.0) (3.9.1)\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from python-Levenshtein->gramformer==1.0) (0.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers->gramformer==1.0) (4.11.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (5.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (8.2.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.9.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (3.0.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (65.6.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (2.4.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (1.0.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<4,>=3.2.0->errant->gramformer==1.0) (0.3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers->gramformer==1.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers->gramformer==1.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers->gramformer==1.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers->gramformer==1.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers->gramformer==1.0) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.18.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->spacy<4,>=3.2.0->errant->gramformer==1.0) (2.1.1)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\vaibhav\\appdata\\local\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4,>=3.2.0->errant->gramformer==1.0) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/PrithivirajDamodaran/Gramformer.git 'C:\\Users\\Vaibhav\\AppData\\Local\\Temp\\pip-req-build-_ywjkkt9'\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85bbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/PrithivirajDamodaran/Gramformer\n",
    "# pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
    "# python -m spacy download en_core_web_sm \n",
    "from gramformer import Gramformer\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16df3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vaibhav\\Desktop\\writing\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Vaibhav\\Desktop\\writing\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vaibhav\\Desktop\\writing\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n"
     ]
    }
   ],
   "source": [
    "gf = Gramformer(models = 1, use_gpu=False) # 1=corrector, 2=detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94177dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_fullforms ={'ADV': 'Adverb', 'PREP': 'Prepositions', 'PRON': 'Pronoun', 'WO': 'Wrong Order', 'VERB': 'Verbs', 'VERB:SVA': 'Singular-Plural', 'VERB:TENSE': 'Verb Tenses', 'VERB:FORM': 'Verb Forms', 'VERB:INFL': 'Verbs', 'SPELL': 'Spelling', 'OTHER': 'Other', 'NOUN': 'Other', 'NOUN:NUM': 'Singular-Plural', 'DET': 'Articles', 'MORPH': 'Other', 'ADJ': 'Adjectives', 'PART': 'Other', 'ORTH': 'Other', 'CONJ': 'Conjugations', 'PUNCT': 'Punctuation'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d567e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strikethrough(text):\n",
    "    result = ''\n",
    "    for c in text:\n",
    "        result = result + c + '\\u0336'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e2296b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ef14d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misspelled_words(input_text):\n",
    "    word_list = [a.lower() for a in word_tokenize(re.sub('\\W+', ' ', input_text))]\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    misspelled = spell.unknown(lemma_words)\n",
    "    return misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4e49700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_spelling_correction(input_sentence):\n",
    "    word_list = [a.lower() for a in word_tokenize(re.sub('\\W+', ' ', input_text))]\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    corrected_words = []\n",
    "    for word in lemma_words:\n",
    "        if len(spell.unknown([word])):\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    corrected_sentence = \" \".join(corrected_words) + \".\"\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77be00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_grammar_correction(input_text):\n",
    "    sentences = sent_tokenize(input_text)\n",
    "    edits = []\n",
    "    corrected_text = ''\n",
    "    color_corrected_text = ''\n",
    "    for sentence in sentences:\n",
    "        corrected_sentences = gf.correct(sentence, max_candidates=1)\n",
    "        for corrected_sentence in corrected_sentences:\n",
    "            all_edits = gf.get_edits(sentence, corrected_sentence)\n",
    "            if len(all_edits):\n",
    "                edits += [a[0] for a in all_edits]\n",
    "                orig = re.split(' ', sentence)\n",
    "                amend = re.split(' ', corrected_sentence)\n",
    "                amend_plus = []\n",
    "                start  = 0\n",
    "                for edit in all_edits:\n",
    "                    #print(edit)\n",
    "                    amend_plus.extend(orig[start:edit[2]])\n",
    "                    if len(edit[1]):\n",
    "                        #amend_plus.extend([strikethrough(edit[1])])\n",
    "                        amend_plus.extend(['<span style=\"background-color:#ffffff;color:#ff3f33\">' + strikethrough(edit[1]) + '</span>'])\n",
    "                    if len(edit[4]):\n",
    "                        #amend_plus.extend([edit[4]])\n",
    "                        amend_plus.extend(['<span style=\"color:#07b81a\">' + edit[4] + '</span>'])\n",
    "                    start = edit[3]\n",
    "                amend_plus.extend(orig[edit[3]:])\n",
    "                color_corrected_sentence = ' '.join(amend_plus)\n",
    "                #print(sentence)\n",
    "                #print(corrected_sentence)\n",
    "                corrected_text += ' ' + corrected_sentence\n",
    "                color_corrected_text += ' ' + color_corrected_sentence\n",
    "            else:\n",
    "                corrected_text += ' ' + sentence\n",
    "                color_corrected_text += ' ' + sentence             \n",
    "    mistake_stats = pd.Series([grammar_fullforms[a] for a in edits]).value_counts()\n",
    "    return corrected_text, color_corrected_text, edits, mistake_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a42981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cefr_ratings(input_text):\n",
    "    nopunc_input_text = re.sub(r'[^\\w\\s]','', input_text.lower())\n",
    "    nopunc_input_text = re.sub(r'[0-9]','', nopunc_input_text)\n",
    "    words = word_tokenize(nopunc_input_text)\n",
    "    lemma_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "    pos_values = ['v', 'a', 'n', 'r', 's']\n",
    "\n",
    "    cefr_list = []\n",
    "    cefr_mapping = {}\n",
    "    for word in lemma_words:\n",
    "        if word in word_set:\n",
    "            cefr_list.append(cefr_dict[word])\n",
    "            cefr_mapping[word] = cefr_dict[word]\n",
    "        else:      \n",
    "            for pos_value in pos_values:\n",
    "                changed_word = lemmatizer.lemmatize(word, pos = pos_value)\n",
    "                if changed_word != word:\n",
    "                    break\n",
    "            if changed_word in word_set:\n",
    "                cefr_list.append(cefr_dict[changed_word])\n",
    "                cefr_mapping[changed_word] = cefr_dict[changed_word]\n",
    "            else:\n",
    "                #print(changed_word)\n",
    "                cefr_list.append('uncategorized')\n",
    "                cefr_mapping[changed_word] = 'uncategorized'\n",
    "    return cefr_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ec045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a9a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_text, color_corrected_text, edits, mistake_stats = text_grammar_correction(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac3ad21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Climate change is <span style=\"background-color:#ffffff;color:#ff3f33\">r̶e̶a̶l̶y̶</span> <span style=\"color:#07b81a\">really</span> a big problem that we all need to pay attention to. The earth is getting warmer and warmer each year. This is <span style=\"background-color:#ffffff;color:#ff3f33\">b̶e̶c̶a̶u̶s̶</span> <span style=\"color:#07b81a\">because</span> of <span style=\"background-color:#ffffff;color:#ff3f33\">a̶l̶o̶t̶</span> <span style=\"color:#07b81a\">a lot</span> of reasons but the biggest reason is human <span style=\"background-color:#ffffff;color:#ff3f33\">a̶c̶t̶i̶v̶i̶t̶e̶s̶.̶</span> <span style=\"color:#07b81a\">activities.</span> We burn to much fossil fuels, like coal and oil, which releases carbon dioxide into the atmosphere. This gas traps heat from the sun and makes the planet hotter. This <span style=\"background-color:#ffffff;color:#ff3f33\">p̶h̶e̶n̶o̶m̶i̶n̶o̶n̶</span> <span style=\"color:#07b81a\">phenomenon</span> is called the greenhouse effect. Another cause of climate change is <span style=\"background-color:#ffffff;color:#ff3f33\">d̶e̶f̶o̶r̶e̶s̶t̶i̶o̶n̶.̶</span> <span style=\"color:#07b81a\">deforestation.</span> Trees absorb carbon dioxide, so when we cut them down, there are fewer trees to soak up this harmful gas. This leads to more carbon dioxide in the atmosphere and more warming. Also, many forests are being destroyed to make broom for agriculture. This is not only bad for the climate but also for the animals that live in these forests. Climate change has many bad <span style=\"background-color:#ffffff;color:#ff3f33\">e̶f̶e̶c̶t̶s̶</span> <span style=\"color:#07b81a\">effects</span> on our planet. For example, it is causing ice caps to melt. This results in rising sea levels, which can lead to flooding in coastal areas. Moreover, the weather is becoming more unpredictable. We are seeing more frequent and severe storms, droughts, and heatwaves. These extreme weather events can cause <span style=\"background-color:#ffffff;color:#ff3f33\">a̶l̶o̶t̶</span> <span style=\"color:#07b81a\">a lot</span> of damage to homes and communities. To fight climate change, we need to make some changes in our lives. We should use less energy and try to use more renewable sources of energy like solar and wind power. We should also try to reduce waste and recycle more. Planting trees can also help a great deal. Everyone can play a part in helping to protect our planet for future generations. It's a big challenge, but it's one we can overcome if we work together."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(color_corrected_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74305533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════╕\n",
      "│          │   count │\n",
      "╞══════════╪═════════╡\n",
      "│ Spelling │       4 │\n",
      "├──────────┼─────────┤\n",
      "│ Other    │       4 │\n",
      "╘══════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(pd.DataFrame(mistake_stats), headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6db705dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════╤═════════╕\n",
      "│               │   count │\n",
      "╞═══════════════╪═════════╡\n",
      "│ B1            │      34 │\n",
      "├───────────────┼─────────┤\n",
      "│ A1            │      33 │\n",
      "├───────────────┼─────────┤\n",
      "│ A2            │      20 │\n",
      "├───────────────┼─────────┤\n",
      "│ B2            │      10 │\n",
      "├───────────────┼─────────┤\n",
      "│ uncategorized │       2 │\n",
      "├───────────────┼─────────┤\n",
      "│ C1            │       1 │\n",
      "╘═══════════════╧═════════╛\n"
     ]
    }
   ],
   "source": [
    "# Process the text using spaCy\n",
    "doc = nlp(corrected_text)\n",
    " \n",
    "# Remove stopwords\n",
    "filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Join the filtered words to form a clean text\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "cefr_mapping = cefr_ratings(clean_text)\n",
    "cefr_df = pd.DataFrame(pd.Series(cefr_mapping.values()).value_counts())\n",
    "print(tabulate(cefr_df, headers = 'keys', tablefmt = 'fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee66eef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heatwaves', 'renewable']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in cefr_mapping.keys() if cefr_mapping[word] == 'uncategorized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f72f403-c1d1-41af-9531-2298902410d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gramformer] Grammar error correct/highlight model loaded..\n",
      "Enter the text to be processed: Climate change is realy a big problem that we all need to pay attention to. The earth is getting warmer and warmer each year. This is becaus of alot of reasons but the biggest reason is human activites. We burn to much fossil fuels, like coal and oil, which releases carbon dioxide into the atmosphere. This gas traps heat from the sun and makes the planet hotter. This phenominon is called the greenhouse effect.  Another cause of climate change is deforestion. Trees absorb carbon dioxide, so when we cut them down, there are fewer trees to soak up this harmful gas. This leads to more carbon dioxide in the atmosphere and more warming. Also, many forests are being destroyed to make room for agriculture. This is not only bad for the climate but also for the animals that live in these forests.  Climate change has many bad efects on our planet. For example, it is causing ice caps to melt. This results in rising sea levels, which can lead to flooding in coastal areas. Moreover, the weather is becoming more unpredictable. We are seeing more frequent and severe storms, droughts, and heatwaves. These extreme weather events can cause alot of damage to homes and communities.  To fight climate change, we need to make some changes in our lives. We should use less energy and try to use more renewable sources of energy like solar and wind power. We should also try to reduce waste and recycle more. Planting trees can also help a great deal. Everyone can play a part in helping to protect our planet for future generations. It's a big challenge, but it's one we can overcome if we work together.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Climate change is <span style=\"background-color:#ffffff;color:#ff3f33\">r̶e̶a̶l̶y̶</span> <span style=\"color:#07b81a\">really</span> a big problem that we all need to pay attention to. The earth is getting warmer and warmer each year. This is <span style=\"background-color:#ffffff;color:#ff3f33\">b̶e̶c̶a̶u̶s̶</span> <span style=\"color:#07b81a\">because</span> of <span style=\"background-color:#ffffff;color:#ff3f33\">a̶l̶o̶t̶</span> <span style=\"color:#07b81a\">a lot</span> of reasons but the biggest reason is human <span style=\"background-color:#ffffff;color:#ff3f33\">a̶c̶t̶i̶v̶i̶t̶e̶s̶.̶</span> <span style=\"color:#07b81a\">activities.</span> We burn to much fossil fuels, like coal and oil, which releases carbon dioxide into the atmosphere. This gas traps heat from the sun and makes the planet hotter. This <span style=\"background-color:#ffffff;color:#ff3f33\">p̶h̶e̶n̶o̶m̶i̶n̶o̶n̶</span> <span style=\"color:#07b81a\">phenomenon</span> is called the greenhouse effect. Another cause of climate change is <span style=\"background-color:#ffffff;color:#ff3f33\">d̶e̶f̶o̶r̶e̶s̶t̶i̶o̶n̶.̶</span> <span style=\"color:#07b81a\">deforestation.</span> Trees absorb carbon dioxide, so when we cut them down, there are fewer trees to soak up this harmful gas. This leads to more carbon dioxide in the atmosphere and more warming. Also, many forests are being destroyed to make room for agriculture. This is not only bad for the climate but also for the animals that live in these forests. Climate change has many bad <span style=\"background-color:#ffffff;color:#ff3f33\">e̶f̶e̶c̶t̶s̶</span> <span style=\"color:#07b81a\">effects</span> on our planet. For example, it <span style=\"background-color:#ffffff;color:#ff3f33\">i̶s̶ ̶c̶a̶u̶s̶i̶n̶g̶</span> <span style=\"color:#07b81a\">causes</span> ice caps to melt. This results in rising sea levels, which can lead to flooding in coastal areas. Moreover, the weather is becoming more unpredictable. We are seeing more frequent and severe storms, droughts, and heatwaves. These extreme weather events can cause <span style=\"background-color:#ffffff;color:#ff3f33\">a̶l̶o̶t̶</span> <span style=\"color:#07b81a\">a lot</span> of damage to homes and communities. To fight climate change, we need to make some changes in our lives. We should use less energy and try to use more renewable sources of energy like solar and wind power. We should also try to reduce waste and recycle more. Planting trees can also help a great deal. Everyone can play a part in helping to protect our planet for future generations. It's a big challenge, but it's one we can overcome if we work together."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════╤═════════╕\n",
      "│             │   count │\n",
      "╞═════════════╪═════════╡\n",
      "│ Spelling    │       4 │\n",
      "├─────────────┼─────────┤\n",
      "│ Other       │       4 │\n",
      "├─────────────┼─────────┤\n",
      "│ Verb Tenses │       1 │\n",
      "╘═════════════╧═════════╛\n",
      "╒═══════════════╤═════════╕\n",
      "│               │   count │\n",
      "╞═══════════════╪═════════╡\n",
      "│ B1            │      34 │\n",
      "├───────────────┼─────────┤\n",
      "│ A1            │      33 │\n",
      "├───────────────┼─────────┤\n",
      "│ A2            │      20 │\n",
      "├───────────────┼─────────┤\n",
      "│ B2            │      10 │\n",
      "├───────────────┼─────────┤\n",
      "│ uncategorized │       2 │\n",
      "├───────────────┼─────────┤\n",
      "│ C1            │       1 │\n",
      "╘═══════════════╧═════════╛\n",
      "Uncategorized words: ['heatwaves', 'renewable']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "from tabulate import tabulate\n",
    "from gramformer import Gramformer\n",
    "import torch\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load CEFR vocabulary\n",
    "cefr_vocab = pd.read_csv('./cefr-vocab-cefrj-octanove.csv')\n",
    "cefr_dict = {k: v for k, v in cefr_vocab[['headword', 'CEFR']].values}\n",
    "word_set = set(cefr_vocab.headword)\n",
    "\n",
    "grammar_fullforms = {'ADV': 'Adverb', 'PREP': 'Prepositions', 'PRON': 'Pronoun', 'WO': 'Wrong Order', 'VERB': 'Verbs',\n",
    "                     'VERB:SVA': 'Singular-Plural', 'VERB:TENSE': 'Verb Tenses', 'VERB:FORM': 'Verb Forms',\n",
    "                     'VERB:INFL': 'Verbs', 'SPELL': 'Spelling', 'OTHER': 'Other', 'NOUN': 'Other', 'NOUN:NUM': 'Singular-Plural',\n",
    "                     'DET': 'Articles', 'MORPH': 'Other', 'ADJ': 'Adjectives', 'PART': 'Other', 'ORTH': 'Other',\n",
    "                     'CONJ': 'Conjugations', 'PUNCT': 'Punctuation'}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Initialize Gramformer\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1212)\n",
    "gf = Gramformer(models=1, use_gpu=False)  # 1=corrector, 2=detector\n",
    "\n",
    "def strikethrough(text):\n",
    "    return ''.join([c + '\\u0336' for c in text])\n",
    "\n",
    "def misspelled_words(input_text):\n",
    "    word_list = [a.lower() for a in word_tokenize(re.sub('\\W+', ' ', input_text))]\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    return spell.unknown(lemma_words)\n",
    "\n",
    "def sentence_spelling_correction(input_sentence):\n",
    "    word_list = [a.lower() for a in word_tokenize(re.sub('\\W+', ' ', input_sentence))]\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    corrected_words = [spell.correction(word) if len(spell.unknown([word])) else word for word in lemma_words]\n",
    "    return \" \".join(corrected_words) + \".\"\n",
    "\n",
    "def text_grammar_correction(input_text):\n",
    "    sentences = sent_tokenize(input_text)\n",
    "    edits = []\n",
    "    corrected_text = ''\n",
    "    color_corrected_text = ''\n",
    "    for sentence in sentences:\n",
    "        corrected_sentences = gf.correct(sentence, max_candidates=1)\n",
    "        for corrected_sentence in corrected_sentences:\n",
    "            all_edits = gf.get_edits(sentence, corrected_sentence)\n",
    "            if len(all_edits):\n",
    "                edits += [a[0] for a in all_edits]\n",
    "                orig = re.split(' ', sentence)\n",
    "                amend = re.split(' ', corrected_sentence)\n",
    "                amend_plus = []\n",
    "                start = 0\n",
    "                for edit in all_edits:\n",
    "                    amend_plus.extend(orig[start:edit[2]])\n",
    "                    if len(edit[1]):\n",
    "                        amend_plus.extend(['<span style=\"background-color:#ffffff;color:#ff3f33\">' + strikethrough(edit[1]) + '</span>'])\n",
    "                    if len(edit[4]):\n",
    "                        amend_plus.extend(['<span style=\"color:#07b81a\">' + edit[4] + '</span>'])\n",
    "                    start = edit[3]\n",
    "                amend_plus.extend(orig[edit[3]:])\n",
    "                color_corrected_sentence = ' '.join(amend_plus)\n",
    "                corrected_text += ' ' + corrected_sentence\n",
    "                color_corrected_text += ' ' + color_corrected_sentence\n",
    "            else:\n",
    "                corrected_text += ' ' + sentence\n",
    "                color_corrected_text += ' ' + sentence            \n",
    "    mistake_stats = pd.Series([grammar_fullforms[a] for a in edits]).value_counts()\n",
    "    return corrected_text.strip(), color_corrected_text.strip(), edits, mistake_stats\n",
    "\n",
    "def cefr_ratings(input_text):\n",
    "    nopunc_input_text = re.sub(r'[^\\w\\s]', '', input_text.lower())\n",
    "    nopunc_input_text = re.sub(r'[0-9]', '', nopunc_input_text)\n",
    "    words = word_tokenize(nopunc_input_text)\n",
    "    lemma_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "    pos_values = ['v', 'a', 'n', 'r', 's']\n",
    "    cefr_list = []\n",
    "    cefr_mapping = {}\n",
    "    for word in lemma_words:\n",
    "        if word in word_set:\n",
    "            cefr_list.append(cefr_dict[word])\n",
    "            cefr_mapping[word] = cefr_dict[word]\n",
    "        else:      \n",
    "            for pos_value in pos_values:\n",
    "                changed_word = lemmatizer.lemmatize(word, pos=pos_value)\n",
    "                if changed_word != word:\n",
    "                    break\n",
    "            if changed_word in word_set:\n",
    "                cefr_list.append(cefr_dict[changed_word])\n",
    "                cefr_mapping[changed_word] = cefr_dict[changed_word]\n",
    "            else:\n",
    "                cefr_list.append('uncategorized')\n",
    "                cefr_mapping[changed_word] = 'uncategorized'\n",
    "    return cefr_mapping\n",
    "\n",
    "def process_text(input_text):\n",
    "    corrected_text, color_corrected_text, edits, mistake_stats = text_grammar_correction(input_text)\n",
    "    display(Markdown(color_corrected_text))\n",
    "    print(tabulate(pd.DataFrame(mistake_stats), headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(corrected_text)\n",
    "    filtered_words = [token.text for token in doc if not token.is_stop]\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "\n",
    "    cefr_mapping = cefr_ratings(clean_text)\n",
    "    cefr_df = pd.DataFrame(pd.Series(cefr_mapping.values()).value_counts())\n",
    "    print(tabulate(cefr_df, headers='keys', tablefmt='fancy_grid'))\n",
    "    return [word for word in cefr_mapping.keys() if cefr_mapping[word] == 'uncategorized']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = input(\"Enter the text to be processed: \")\n",
    "    uncategorized_words = process_text(input_text)\n",
    "    print(\"Uncategorized words:\", uncategorized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed7029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
